{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86c803d87eee479392486e46934ceada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_cd4a86d3d68146b49dfb03d051eabf2d"
          }
        },
        "0e55c9d74537448a839e1ee4a5493631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_186fbb5d17504464af24cc4b37e8d7e9",
            "placeholder": "​",
            "style": "IPY_MODEL_41a4800b5e634809966a5e9d1ea19732",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "dbf088929f3442a18fb7368db1a64ac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_76a31b34201b4c4b8f55225b4c4aa1fa",
            "placeholder": "​",
            "style": "IPY_MODEL_925acfaa040840ac9a1fc9dae4b7c949",
            "value": ""
          }
        },
        "bf831a341b9841d3886a534c42e33881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_70d1da87c01946a9ab060973164f1cb3",
            "style": "IPY_MODEL_a7bf9a2efd79445abf94ba2406b0d22e",
            "value": true
          }
        },
        "44b8bb9c539b468e978ef42acfd05aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b3560d3b7ba944a394c95e03b1ac9921",
            "style": "IPY_MODEL_4099faefac39414a9e249157eb530d9d",
            "tooltip": ""
          }
        },
        "0970d82e0896489ab4578dbf618c6cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3997bf49095d49b692787188b3b182f1",
            "placeholder": "​",
            "style": "IPY_MODEL_569bebf6682b4c4d915d405d0de850d6",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "cd4a86d3d68146b49dfb03d051eabf2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "186fbb5d17504464af24cc4b37e8d7e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41a4800b5e634809966a5e9d1ea19732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76a31b34201b4c4b8f55225b4c4aa1fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "925acfaa040840ac9a1fc9dae4b7c949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70d1da87c01946a9ab060973164f1cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7bf9a2efd79445abf94ba2406b0d22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3560d3b7ba944a394c95e03b1ac9921": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4099faefac39414a9e249157eb530d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3997bf49095d49b692787188b3b182f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "569bebf6682b4c4d915d405d0de850d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "943d6004e5d54dc29ecfcda835036864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8567d15d9ea74bc9a6d0c9b433fa182f",
            "placeholder": "​",
            "style": "IPY_MODEL_76c683a82aa6419f95ee24fcad4ee567",
            "value": "Connecting..."
          }
        },
        "8567d15d9ea74bc9a6d0c9b433fa182f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76c683a82aa6419f95ee24fcad4ee567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Call Quality Analyser\n",
        "### This notebook is using google T4 GPU so please change the runtime before executing any code\n",
        "### You also need to add your google api key with the name as GOOGLE_API_KEY in secerets for running the insights part of the code\n",
        "### This notebook is divided into section for each process in the pipeline\n",
        "#### Downloading audio from youtube\n",
        "#### Preprocessing audio\n",
        "#### Transcription(audio to text)\n",
        "#### Diarisation\n",
        "#### Call Metrics\n",
        "#### Sentiment Analysis\n",
        "#### Actionable Insight\n",
        "#### In the end the the complete pipeline is timed which is under 15 seconds\n",
        "## I advise to run the notebook in order to avoid any errors.\n"
      ],
      "metadata": {
        "id": "usPgnWibIkgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing necessary libraries"
      ],
      "metadata": {
        "id": "QT9C3K_DgbBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install libcudnn8 libcudnn8-dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU2vn1IKfem6",
        "outputId": "00596a8f-e5e1-4f04-f92e-68e84677f0cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libcudnn8 libcudnn8-dev\n",
            "0 upgraded, 2 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 885 MB of archives.\n",
            "After this operation, 2,380 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcudnn8 8.9.7.29-1+cuda12.2 [444 MB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcudnn8-dev 8.9.7.29-1+cuda12.2 [440 MB]\n",
            "Fetched 885 MB in 19s (47.6 MB/s)\n",
            "Selecting previously unselected package libcudnn8.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.9.7.29-1+cuda12.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.9.7.29-1+cuda12.2) ...\n",
            "Selecting previously unselected package libcudnn8-dev.\n",
            "Preparing to unpack .../libcudnn8-dev_8.9.7.29-1+cuda12.2_amd64.deb ...\n",
            "Unpacking libcudnn8-dev (8.9.7.29-1+cuda12.2) ...\n",
            "Setting up libcudnn8 (8.9.7.29-1+cuda12.2) ...\n",
            "Setting up libcudnn8-dev (8.9.7.29-1+cuda12.2) ...\n",
            "update-alternatives: warning: forcing reinstallation of alternative /usr/include/x86_64-linux-gnu/cudnn_v9.h because link group libcudnn is broken\n",
            "update-alternatives: using /usr/include/x86_64-linux-gnu/cudnn_v8.h to provide /usr/include/cudnn.h (libcudnn) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U yt-dlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1Td2jGrgvX7",
        "outputId": "08cc79f7-c7a4-4429-9e1a-fca1870562c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.9.5-py3-none-any.whl.metadata (177 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/177.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2025.9.5-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2025.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q git+https://github.com/openai/whisper.git\n",
        "# use for whisper pipeline"
      ],
      "metadata": {
        "id": "Iq4a-XiUSv8u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa soundfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL3OVn5sahl6",
        "outputId": "3bff33ad-4540-4d38-8823-9513d86b4891"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.4.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install whisperx\n",
        "!pip install -q google-generativeai\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF4CkgoUfJl4",
        "outputId": "85064f10-3dab-4852-d41d-219320fb541e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting whisperx\n",
            "  Downloading whisperx-3.4.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting ctranslate2<4.5.0 (from whisperx)\n",
            "  Downloading ctranslate2-4.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting faster-whisper>=1.1.1 (from whisperx)\n",
            "  Downloading faster_whisper-1.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: nltk>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from whisperx) (3.9.1)\n",
            "Requirement already satisfied: numpy>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from whisperx) (2.0.2)\n",
            "Collecting onnxruntime>=1.19 (from whisperx)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting pandas>=2.2.3 (from whisperx)\n",
            "  Downloading pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote-audio>=3.3.2 (from whisperx)\n",
            "  Downloading pyannote_audio-3.4.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from whisperx) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from whisperx) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.48.0 in /usr/local/lib/python3.12/dist-packages (from whisperx) (4.56.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from ctranslate2<4.5.0->whisperx) (75.2.0)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.12/dist-packages (from ctranslate2<4.5.0->whisperx) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper>=1.1.1->whisperx) (0.34.4)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper>=1.1.1->whisperx) (0.22.0)\n",
            "Collecting av>=11 (from faster-whisper>=1.1.1->whisperx)\n",
            "  Downloading av-15.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from faster-whisper>=1.1.1->whisperx) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9.1->whisperx) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9.1->whisperx) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9.1->whisperx) (2024.11.6)\n",
            "Collecting coloredlogs (from onnxruntime>=1.19->whisperx)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19->whisperx) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19->whisperx) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19->whisperx) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19->whisperx) (1.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->whisperx) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->whisperx) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->whisperx) (2025.2)\n",
            "Collecting asteroid-filterbanks>=0.4 (from pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pyannote-audio>=3.3.2->whisperx) (0.8.1)\n",
            "Collecting lightning>=2.0.1 (from pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading lightning-2.5.5-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from pyannote-audio>=3.3.2->whisperx) (2.3.0)\n",
            "Collecting pyannote.core<6.0,>=5.0.0 (from pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyannote.database<6.0,>=5.0.1 (from pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pyannote.metrics<4.0,>=3.2 (from pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pyannote.pipeline<4.0,>=3.0.1 (from pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n",
            "Collecting pytorch_metric_learning>=2.1.0 (from pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading pytorch_metric_learning-2.9.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from pyannote-audio>=3.3.2->whisperx) (13.9.4)\n",
            "Collecting semver>=3.0.0 (from pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from pyannote-audio>=3.3.2->whisperx) (0.13.1)\n",
            "Collecting speechbrain>=1.0.0 (from pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tensorboardX>=2.6 (from pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting torch_audiomentations>=0.11.0 (from pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting torchmetrics>=0.11.0 (from pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->whisperx) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.48.0->whisperx) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.48.0->whisperx) (0.6.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper>=1.1.1->whisperx) (1.1.9)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting pytorch-lightning (from lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading pytorch_lightning-2.5.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf<3.0,>=2.1->pyannote-audio>=3.3.2->whisperx) (4.9.3)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from pyannote.core<6.0,>=5.0.0->pyannote-audio>=3.3.2->whisperx) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.core<6.0,>=5.0.0->pyannote-audio>=3.3.2->whisperx) (1.16.1)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.database<6.0,>=5.0.1->pyannote-audio>=3.3.2->whisperx) (0.17.3)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.metrics<4.0,>=3.2->pyannote-audio>=3.3.2->whisperx) (1.6.1)\n",
            "Collecting docopt>=0.6.2 (from pyannote.metrics<4.0,>=3.2->pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from pyannote.metrics<4.0,>=3.2->pyannote-audio>=3.3.2->whisperx) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.metrics<4.0,>=3.2->pyannote-audio>=3.3.2->whisperx) (3.10.0)\n",
            "Collecting optuna>=3.1 (from pyannote.pipeline<4.0,>=3.0.1->pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.3->whisperx) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.0.0->pyannote-audio>=3.3.2->whisperx) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.0.0->pyannote-audio>=3.3.2->whisperx) (2.19.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->pyannote-audio>=3.3.2->whisperx) (1.17.1)\n",
            "Collecting hyperpyyaml (from speechbrain>=1.0.0->pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from speechbrain>=1.0.0->pyannote-audio>=3.3.2->whisperx) (0.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.19->whisperx) (1.3.0)\n",
            "Collecting julius<0.3,>=0.2.3 (from torch_audiomentations>=0.11.0->pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-pitch-shift>=1.2.2 (from torch_audiomentations>=0.11.0->pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.19->whisperx)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.5.1->whisperx) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.48.0->whisperx) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.48.0->whisperx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.48.0->whisperx) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.48.0->whisperx) (2025.8.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote-audio>=3.3.2->whisperx) (2.22)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (3.12.15)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote-audio>=3.3.2->whisperx) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote-audio>=3.3.2->whisperx) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote-audio>=3.3.2->whisperx) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote-audio>=3.3.2->whisperx) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote-audio>=3.3.2->whisperx) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote-audio>=3.3.2->whisperx) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote-audio>=3.3.2->whisperx) (3.2.3)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio>=3.3.2->whisperx) (1.16.5)\n",
            "Collecting colorlog (from optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio>=3.3.2->whisperx) (2.0.43)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics<4.0,>=3.2->pyannote-audio>=3.3.2->whisperx) (3.6.0)\n",
            "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch_audiomentations>=0.11.0->pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.12.1->pyannote.database<6.0,>=5.0.1->pyannote-audio>=3.3.2->whisperx) (1.5.4)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=1.0.0->pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading ruamel.yaml-0.18.15-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (1.20.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio>=3.3.2->whisperx) (1.3.10)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote-audio>=3.3.2->whisperx)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio>=3.3.2->whisperx) (3.2.4)\n",
            "Downloading whisperx-3.4.2-py3-none-any.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faster_whisper-1.2.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote_audio-3.4.0-py2.py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.8/897.8 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
            "Downloading av-15.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.5-py3-none-any.whl (828 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.5/828.5 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading pytorch_metric_learning-2.9.0-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
            "Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n",
            "Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading pytorch_lightning-2.5.5-py3-none-any.whl (832 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.4/832.4 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Downloading ruamel.yaml-0.18.15-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading ruamel.yaml.clib-0.2.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (754 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m754.1/754.1 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt, julius\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=89d0393e21a679dc89e915c537085a330d8de65d837ff664c36b41d351165a56\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=375ef4bdcf7c0068c8e378e3eafb9ab4b3c2d9ebfccbd5f04e9d7110a58aea51\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c1/ca/544dafe48401e8e2e17064dfe465a390fca9e8720ffa12e744\n",
            "Successfully built docopt julius\n",
            "Installing collected packages: primePy, docopt, tensorboardX, semver, ruamel.yaml.clib, lightning-utilities, humanfriendly, ctranslate2, colorlog, av, ruamel.yaml, pyannote.core, pandas, coloredlogs, optuna, onnxruntime, hyperpyyaml, torchmetrics, pytorch_metric_learning, pyannote.database, julius, faster-whisper, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch_audiomentations, lightning, pyannote-audio, whisperx\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.2 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asteroid-filterbanks-0.4.0 av-15.1.0 coloredlogs-15.0.1 colorlog-6.9.0 ctranslate2-4.4.0 docopt-0.6.2 faster-whisper-1.2.0 humanfriendly-10.0 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.5 lightning-utilities-0.15.2 onnxruntime-1.22.1 optuna-4.5.0 pandas-2.3.2 primePy-1.3 pyannote-audio-3.4.0 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.5.5 pytorch_metric_learning-2.9.0 ruamel.yaml-0.18.15 ruamel.yaml.clib-0.2.12 semver-3.0.4 speechbrain-1.0.3 tensorboardX-2.6.4 torch-pitch-shift-1.2.5 torch_audiomentations-0.12.0 torchmetrics-1.8.2 whisperx-3.4.2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THIS IS NECESSARY\n",
        "## whisperx uses pyannote for diarisation\n",
        "## please go to hugging face website and login/signup\n",
        "## then search for pyannote/speaker-diarisation and provide your details to access this gated model\n",
        "## and similarly do for pyannote/segmentation\n",
        "## links: https://huggingface.co/pyannote/segmentation-3.0\n",
        "##        https://huggingface.co/pyannote/speaker-diarization-3.1\n",
        "## After this generate a hugging face token with read access and save it somewhere\n",
        "## Enter that token here"
      ],
      "metadata": {
        "id": "3wCi1yiyd7xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THIS IS NECESSARY\n",
        "# whisperx uses pyannote for diarisation\n",
        "# please go to hugging face website and login/signup\n",
        "# then search for pyannote/speaker-diarisation and provide your details to access this gated model\n",
        "# and similarly do for pyannote/segmentation\n",
        "# links: https://huggingface.co/pyannote/segmentation-3.0\n",
        "#        https://huggingface.co/pyannote/speaker-diarization-3.1\n",
        "# After this generate a hugging face token with read access and save it somewhere\n",
        "# Enter that token here\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "86c803d87eee479392486e46934ceada",
            "0e55c9d74537448a839e1ee4a5493631",
            "dbf088929f3442a18fb7368db1a64ac7",
            "bf831a341b9841d3886a534c42e33881",
            "44b8bb9c539b468e978ef42acfd05aed",
            "0970d82e0896489ab4578dbf618c6cd2",
            "cd4a86d3d68146b49dfb03d051eabf2d",
            "186fbb5d17504464af24cc4b37e8d7e9",
            "41a4800b5e634809966a5e9d1ea19732",
            "76a31b34201b4c4b8f55225b4c4aa1fa",
            "925acfaa040840ac9a1fc9dae4b7c949",
            "70d1da87c01946a9ab060973164f1cb3",
            "a7bf9a2efd79445abf94ba2406b0d22e",
            "b3560d3b7ba944a394c95e03b1ac9921",
            "4099faefac39414a9e249157eb530d9d",
            "3997bf49095d49b692787188b3b182f1",
            "569bebf6682b4c4d915d405d0de850d6",
            "943d6004e5d54dc29ecfcda835036864",
            "8567d15d9ea74bc9a6d0c9b433fa182f",
            "76c683a82aa6419f95ee24fcad4ee567"
          ]
        },
        "id": "Hw3FohJnaoBZ",
        "outputId": "4fa6288b-bfeb-4f58-fb6b-69013052b904"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86c803d87eee479392486e46934ceada"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask the user for their Hugging Face token\n",
        "auth_token = input(\"Please enter your Hugging Face access token: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbiXfFLpfXMy",
        "outputId": "218a37c6-2cf8-4c45-8d55-ca72f51f6dca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your Hugging Face access token: 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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Audio from Youtube"
      ],
      "metadata": {
        "id": "E6OPosmyGipc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yt_dlp\n",
        "import os\n",
        "\n",
        "def download_audio_from_youtube(url, output_dir=\"downloads\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = os.path.join(output_dir, \"%(id)s.%(ext)s\")\n",
        "\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'outtmpl': output_path,\n",
        "        'quiet': True,\n",
        "        'postprocessors': [\n",
        "            {'key': 'FFmpegExtractAudio', 'preferredcodec': 'wav', 'preferredquality': '192'},\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(url, download=True)\n",
        "        audio_path = os.path.join(output_dir, f\"{info['id']}.wav\")\n",
        "\n",
        "    return audio_path"
      ],
      "metadata": {
        "id": "zItJwWfXG4n9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "youtube_url = \"https://youtu.be/4ostqJD3Psc\"  # enter your youtube url. The URL MUST be public for yt-dlp to download it on google Colab\n",
        "\n",
        "raw_audio = download_audio_from_youtube(youtube_url)\n",
        "print(f\"Raw audio saved at: {raw_audio}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJa5JezPHOvA",
        "outputId": "1131a96f-95fd-4f5e-e46b-1c158ab63488"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw audio saved at: downloads/4ostqJD3Psc.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This method sometimes does not work as google colab does not have access to browser cookies in that case i have provided the audio file please upload it on google colab and run the below command"
      ],
      "metadata": {
        "id": "qDVTvYaOhwOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/4ostqJD3Psc.wav /content/downloads/"
      ],
      "metadata": {
        "id": "X40ajQSehu6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157809bf-78b4-45e0-d6ea-a63e05ec9a41"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/4ostqJD3Psc.wav': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio Pre Processing"
      ],
      "metadata": {
        "id": "EqQfv07CJqjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Method 1: The torchaudio-based preprocessing function\n",
        "# def preprocess_audio_torchaudio(input_wav, target_sr=16000):\n",
        "#     # Load audio\n",
        "#     audio, sr = torchaudio.load(input_wav)\n",
        "\n",
        "#     # Convert to mono\n",
        "#     if audio.shape[0] > 1:\n",
        "#         audio = torch.mean(audio, dim=0, keepdim=True)\n",
        "\n",
        "#     # Resample\n",
        "#     if sr != target_sr:\n",
        "#         resampler = torchaudio.transforms.Resample(sr, target_sr)\n",
        "#         audio = resampler(audio)\n",
        "\n",
        "#     # Normalize\n",
        "#     audio = audio / audio.abs().max()\n",
        "\n",
        "#     # Save preprocessed file\n",
        "#     preprocessed_path = input_wav.replace(\".wav\", f\"_preprocessed_{target_sr}.wav\")\n",
        "#     torchaudio.save(preprocessed_path, audio, target_sr)\n",
        "\n",
        "#     return preprocessed_path"
      ],
      "metadata": {
        "id": "SiPZl5-NOoC-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: The more efficient librosa-based preprocessing function\n",
        "def preprocess_audio_librosa(input_wav, target_sr=16000):\n",
        "    # Load, resample, and convert to mono in one step\n",
        "    audio, sr = librosa.load(input_wav, sr=target_sr, mono=True)\n",
        "\n",
        "    # Trim silence\n",
        "    audio, _ = librosa.effects.trim(audio)\n",
        "\n",
        "    # Convert to a PyTorch tensor\n",
        "    audio_tensor = torch.from_numpy(audio)\n",
        "\n",
        "    # Save preprocessed file\n",
        "    preprocessed_path = input_wav.replace(\".wav\", f\"_preprocessed_{target_sr}_librosa.wav\")\n",
        "    sf.write(preprocessed_path, audio, target_sr)\n",
        "\n",
        "    return preprocessed_path\n"
      ],
      "metadata": {
        "id": "dLKvUUnaO1vd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Librosa method is faster. It is slow for the first time because of python's Just In Time compiler"
      ],
      "metadata": {
        "id": "lb_OFSRfeZRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enter your downloaded audio path\n",
        "audio_file_path = '/content/downloads/4ostqJD3Psc.wav'\n",
        "\n",
        "# --- Test the torchaudio method ---\n",
        "# start_time_torch = time.time()\n",
        "# preprocess_audio_torchaudio(audio_file_path)\n",
        "# end_time_torch = time.time()\n",
        "# torch_duration = end_time_torch - start_time_torch\n",
        "# print(f\"torchaudio method took: {torch_duration:.4f} seconds\")\n",
        "\n",
        "# --- Test the librosa method ---\n",
        "# start_time_librosa = time.time()\n",
        "# preprocess_audio_librosa(audio_file_path)\n",
        "# end_time_librosa = time.time()\n",
        "# librosa_duration = end_time_librosa - start_time_librosa\n",
        "# print(f\"librosa method took: {librosa_duration:.4f} seconds\")"
      ],
      "metadata": {
        "id": "OWnd8_4dO85Z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if torch_duration < librosa_duration:\n",
        "#     print(\"The torchaudio method was faster.\")\n",
        "# else:\n",
        "#     print(\"The librosa method was faster.\")"
      ],
      "metadata": {
        "id": "y6mZ2SPQPSQS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_audio = preprocess_audio_librosa(audio_file_path)"
      ],
      "metadata": {
        "id": "yzXIUs6qUPMz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribing Audio to Text"
      ],
      "metadata": {
        "id": "122ty_71S7VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DID NOT CONTINUE WITH THIS BECAUSE THE SEGMENTS USED BY WHISPER WERE TOO LARGE\n",
        "# import whisper\n",
        "# import torch\n",
        "# import librosa\n",
        "# import time\n",
        "\n",
        "# # Load the small Whisper model\n",
        "# print(\"Loading the Whisper model...\")\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model = whisper.load_model(\"small\", device=device)\n",
        "# print(\"Whisper model loaded.\")\n",
        "\n",
        "# # Load the preprocessed audio from the file path\n",
        "# audio, sr = librosa.load(preprocessed_audio, sr=16000)  # Assuming 16000 is the target sample rate\n",
        "\n",
        "# # Measure transcription time\n",
        "# start_time = time.time()\n",
        "# result = model.transcribe(audio, fp16=False)\n",
        "# end_time = time.time()\n",
        "\n",
        "# print(f\"Transcription complete in {end_time - start_time:.2f} seconds.\\n\")\n",
        "\n",
        "# # The 'result' dictionary contains the transcription and timestamps\n",
        "# print(\"\\n--- Transcription ---\")\n",
        "# print(result[\"text\"])\n"
      ],
      "metadata": {
        "id": "_kMrU43pRmGm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyannote.audio\n",
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ],
      "metadata": {
        "id": "QXdrKzyck0mH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPERX USES PYANNOTE DIRECTLY\n",
        "\n",
        "# from pyannote.audio import Pipeline\n",
        "# import torch\n",
        "# import time\n",
        "# import warnings\n",
        "\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# # Ask the user for their Hugging Face token\n",
        "# auth_token = input(\"Please enter your Hugging Face access token: \")\n",
        "\n",
        "# # Load the speaker diarization pipeline\n",
        "# print(\"Loading speaker diarization pipeline...\")\n",
        "# diarization_pipeline = Pipeline.from_pretrained(\n",
        "#     \"pyannote/speaker-diarization-3.1\",\n",
        "#     use_auth_token=auth_token\n",
        "# ).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "# print(\"Pipeline loaded.\")\n",
        "\n",
        "# # Run the diarization on your preprocessed audio file\n",
        "# print(\"Starting speaker diarization...\")\n",
        "# start_time = time.time()\n",
        "# diarization_result = diarization_pipeline(preprocessed_audio)\n",
        "# end_time = time.time()\n",
        "\n",
        "# print(f\"Diarisation complete in {end_time - start_time:.2f} seconds.\\n\")\n",
        "\n",
        "# # You can print the result to see the speaker segments\n",
        "# print(\"\\n--- Speaker Segments ---\")\n",
        "# for segment in diarization_result.itertracks(yield_label=True):\n",
        "#     turn, _, speaker = segment\n",
        "#     print(f\"start={turn.start:.2f}s stop={turn.end:.2f}s speaker={speaker}\")"
      ],
      "metadata": {
        "id": "b7o9qolMUf63"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Extract speaker turns from pyannote result for easier lookup\n",
        "# diarization_turns = []\n",
        "# for turn, _, speaker in diarization_result.itertracks(yield_label=True):\n",
        "#     diarization_turns.append({'start': turn.start, 'end': turn.end, 'speaker': speaker})\n",
        "\n",
        "# # Combine Whisper and Pyannote results\n",
        "# combined_transcript = []\n",
        "\n",
        "# # Iterate through each segment from the Whisper transcription\n",
        "# for whisper_segment in result['segments']:\n",
        "#     segment_start = whisper_segment['start']\n",
        "#     segment_end = whisper_segment['end']\n",
        "\n",
        "#     best_speaker = \"Unknown\"\n",
        "#     max_overlap = 0.0\n",
        "\n",
        "#     # For each Whisper segment, find the PyAnnote turn with the most overlap\n",
        "#     for turn in diarization_turns:\n",
        "#         # Calculate the start and end of the overlap window\n",
        "#         overlap_start = max(segment_start, turn['start'])\n",
        "#         overlap_end = min(segment_end, turn['end'])\n",
        "\n",
        "#         # Calculate the duration of the overlap\n",
        "#         overlap_duration = max(0, overlap_end - overlap_start)\n",
        "\n",
        "#         # If this turn has more overlap than the previous ones, it's our new best match\n",
        "#         if overlap_duration > max_overlap:\n",
        "#             max_overlap = overlap_duration\n",
        "#             best_speaker = turn['speaker']\n",
        "\n",
        "#     # Append the combined information to your new transcript list\n",
        "#     combined_transcript.append({\n",
        "#         'start': segment_start,\n",
        "#         'end': segment_end,\n",
        "#         'text': whisper_segment['text'],\n",
        "#         'speaker': best_speaker\n",
        "#     })\n",
        "\n",
        "# # Print the more accurately combined transcript\n",
        "# print(\"\\n--- Combined Transcript (with improved speaker attribution) ---\")\n",
        "# for segment in combined_transcript:\n",
        "#     print(f\"[{segment['start']:.2f}s - {segment['end']:.2f}s] {segment['speaker']}: {segment['text']}\")"
      ],
      "metadata": {
        "id": "Q3oHsI2SoMKA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3yGPUm6TrJmo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decided to go with whisperx for accurate and combined functionality"
      ],
      "metadata": {
        "id": "T5KUmrlweGIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisperx\n",
        "import torch\n",
        "import time\n",
        "\n",
        "\n",
        "# Select device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "\n",
        "# Load WhisperX ASR model\n",
        "whisper_model = whisperx.load_model(\"small\", device, compute_type=compute_type)\n",
        "\n",
        "align_model = None\n",
        "align_metadata = None\n",
        "\n",
        "hf_token = auth_token\n",
        "diarize_model = whisperx.diarize.DiarizationPipeline(\n",
        "    use_auth_token=hf_token,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "def transcribe_and_diarize(audio_file: str, whisper_model, diarize_model):\n",
        "    \"\"\"\n",
        "    Transcribes and diarizes an audio file using preloaded WhisperX + diarization models.\n",
        "\n",
        "    Args:\n",
        "        audio_file (str): Path to audio file\n",
        "        whisper_model: Preloaded WhisperX ASR model\n",
        "        diarize_model: Preloaded diarization model\n",
        "\n",
        "    Returns:\n",
        "        dict: ASR + diarization results with speaker labels\n",
        "    \"\"\"\n",
        "    # 2. Transcribe\n",
        "    asr_result = whisper_model.transcribe(audio_file)\n",
        "\n",
        "    # 3. Load alignment model if not already loaded\n",
        "    global align_model, align_metadata\n",
        "    if align_model is None:\n",
        "        align_model, align_metadata = whisperx.load_align_model(\n",
        "            language_code=asr_result[\"language\"],\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "    # 4. Align for accurate word timestamps\n",
        "    asr_result_aligned = whisperx.align(\n",
        "        asr_result[\"segments\"],\n",
        "        align_model,\n",
        "        align_metadata,\n",
        "        audio_file,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # 5. Diarization\n",
        "    diarize_segments = diarize_model(audio_file)\n",
        "\n",
        "    # 6. Assign speaker labels\n",
        "    asr_result_with_speakers = whisperx.assign_word_speakers(\n",
        "        diarize_segments,\n",
        "        asr_result_aligned\n",
        "    )\n",
        "\n",
        "    return asr_result_with_speakers\n",
        "\n",
        "\n",
        "audio_file = \"/content/downloads/4ostqJD3Psc_preprocessed_16000_librosa.wav\"\n",
        "result = transcribe_and_diarize(audio_file, whisper_model, diarize_model)\n",
        "\n",
        "# Print transcript in \"Speaker: text\" format\n",
        "for seg in result[\"segments\"]:\n",
        "    print(f\"{seg['speaker']}: {seg['text'].strip()}\")\n"
      ],
      "metadata": {
        "id": "JPbv-o1lQgLC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0333c46-1fdf-4bdd-ee49-4cfb87062b38"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../usr/local/lib/python3.12/dist-packages/whisperx/assets/pytorch_model.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            ">>Performing voice activity detection using Pyannote...\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cu126. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.99) in first 30s of audio...\n",
            "SPEAKER_01: Thank you for calling Nissan.\n",
            "SPEAKER_01: My name is Lauren.\n",
            "SPEAKER_01: Can I have your name?\n",
            "SPEAKER_00: Yeah, my name is John Smith.\n",
            "SPEAKER_01: Thank you, John.\n",
            "SPEAKER_01: How can I help you?\n",
            "SPEAKER_00: I was just calling about to see how much it would cost to update the map in my car.\n",
            "SPEAKER_01: I'd be happy to help you with that today.\n",
            "SPEAKER_01: Did you receive a mailer from us?\n",
            "SPEAKER_00: I did.\n",
            "SPEAKER_00: Do you need the customer number?\n",
            "SPEAKER_01: Yes, please.\n",
            "SPEAKER_00: OK.\n",
            "SPEAKER_00: It's 15243.\n",
            "SPEAKER_00: Thank you.\n",
            "SPEAKER_01: And the year-making model of your vehicle?\n",
            "SPEAKER_00: Yeah, I have a 2009 Nissan Altima.\n",
            "SPEAKER_01: Oh, nice car.\n",
            "SPEAKER_00: Yeah, thank you.\n",
            "SPEAKER_00: We really enjoy it.\n",
            "SPEAKER_01: Okay, I think I found your profile here.\n",
            "SPEAKER_01: Can I have you verify your address and phone number, please?\n",
            "SPEAKER_00: Yes.\n",
            "SPEAKER_00: It's 1255 North Research Way.\n",
            "SPEAKER_00: That's in Orham, Utah, 84097.\n",
            "SPEAKER_00: And my phone number is 8014311000.\n",
            "SPEAKER_00: Thanks, John.\n",
            "SPEAKER_01: I located your information.\n",
            "SPEAKER_01: The newest version we have available for your vehicle is version 7.7, which was released in March of 2012.\n",
            "SPEAKER_01: The price of the new map is $99 plus shipping and tax.\n",
            "SPEAKER_01: Let me go ahead and set up this order for you.\n",
            "SPEAKER_00: Well, can we wait just a second?\n",
            "SPEAKER_00: I'm not really sure if I can afford it right now.\n",
            "SPEAKER_01: All right.\n",
            "SPEAKER_01: Well, here are a few reasons to consider purchasing today.\n",
            "SPEAKER_01: It looks as though you haven't updated your vehicle for three years.\n",
            "SPEAKER_01: So that would be the equivalent of getting three years worth of updates for the price of one.\n",
            "SPEAKER_00: Oh, OK.\n",
            "SPEAKER_01: In addition, special offers like the current promotion don't come around too often.\n",
            "SPEAKER_01: I would definitely recommend taking advantage of the extra $50 off before it expires.\n",
            "SPEAKER_00: Yeah, that does sound pretty good.\n",
            "SPEAKER_01: If I set this order up for you now, it'll ship out today and for $50 less.\n",
            "SPEAKER_01: Do you have your credit card handy?\n",
            "SPEAKER_01: And I can place this order for you now.\n",
            "SPEAKER_00: Yeah, let's go ahead and use a visa.\n",
            "SPEAKER_00: for number eight.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus task : Identifying Sales Rep vs Customer"
      ],
      "metadata": {
        "id": "GO9u9sP_R4mH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_conversation(asr_result_with_speakers: dict) -> str:\n",
        "    \"\"\"\n",
        "    Formats diarized ASR result into a readable conversation with custom speaker labels.\n",
        "\n",
        "    Args:\n",
        "        asr_result_with_speakers (dict): Output from WhisperX with diarization\n",
        "\n",
        "    Returns:\n",
        "        str: Conversation in \"Speaker: text\" format\n",
        "    \"\"\"\n",
        "    speaker_map = {}\n",
        "\n",
        "    # Build speaker mapping\n",
        "    for segment in asr_result_with_speakers[\"segments\"]:\n",
        "        current_speaker = segment.get('speaker')\n",
        "        if current_speaker not in speaker_map:\n",
        "            if len(speaker_map) == 0:\n",
        "                speaker_map[current_speaker] = \"Sales Rep\"\n",
        "            elif len(speaker_map) == 1:\n",
        "                speaker_map[current_speaker] = \"Customer\"\n",
        "            else:\n",
        "                speaker_map[current_speaker] = f\"Other Speaker {len(speaker_map) - 1}\"\n",
        "\n",
        "    # Replace original speakers with mapped labels\n",
        "    for segment in asr_result_with_speakers[\"segments\"]:\n",
        "        original_speaker = segment.get('speaker')\n",
        "        if original_speaker in speaker_map:\n",
        "            segment['speaker'] = speaker_map[original_speaker]\n",
        "\n",
        "    # Build formatted conversation\n",
        "    formatted_conversation = []\n",
        "    for seg in asr_result_with_speakers[\"segments\"]:\n",
        "        speaker = seg[\"speaker\"]\n",
        "        text = seg[\"text\"].strip()\n",
        "        formatted_conversation.append(f\"{speaker}: {text}\")\n",
        "\n",
        "    return \"\\n\".join(formatted_conversation)\n"
      ],
      "metadata": {
        "id": "74nQDJt2PMM0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_string = format_conversation(result)\n",
        "\n",
        "print(conversation_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PjsehBwQvAT",
        "outputId": "288dfcee-a1a3-429b-dd85-e23edfb71e4c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sales Rep: Thank you for calling Nissan.\n",
            "Sales Rep: My name is Lauren.\n",
            "Sales Rep: Can I have your name?\n",
            "Customer: Yeah, my name is John Smith.\n",
            "Sales Rep: Thank you, John.\n",
            "Sales Rep: How can I help you?\n",
            "Customer: I was just calling about to see how much it would cost to update the map in my car.\n",
            "Sales Rep: I'd be happy to help you with that today.\n",
            "Sales Rep: Did you receive a mailer from us?\n",
            "Customer: I did.\n",
            "Customer: Do you need the customer number?\n",
            "Sales Rep: Yes, please.\n",
            "Customer: OK.\n",
            "Customer: It's 15243.\n",
            "Customer: Thank you.\n",
            "Sales Rep: And the year-making model of your vehicle?\n",
            "Customer: Yeah, I have a 2009 Nissan Altima.\n",
            "Sales Rep: Oh, nice car.\n",
            "Customer: Yeah, thank you.\n",
            "Customer: We really enjoy it.\n",
            "Sales Rep: Okay, I think I found your profile here.\n",
            "Sales Rep: Can I have you verify your address and phone number, please?\n",
            "Customer: Yes.\n",
            "Customer: It's 1255 North Research Way.\n",
            "Customer: That's in Orham, Utah, 84097.\n",
            "Customer: And my phone number is 8014311000.\n",
            "Customer: Thanks, John.\n",
            "Sales Rep: I located your information.\n",
            "Sales Rep: The newest version we have available for your vehicle is version 7.7, which was released in March of 2012.\n",
            "Sales Rep: The price of the new map is $99 plus shipping and tax.\n",
            "Sales Rep: Let me go ahead and set up this order for you.\n",
            "Customer: Well, can we wait just a second?\n",
            "Customer: I'm not really sure if I can afford it right now.\n",
            "Sales Rep: All right.\n",
            "Sales Rep: Well, here are a few reasons to consider purchasing today.\n",
            "Sales Rep: It looks as though you haven't updated your vehicle for three years.\n",
            "Sales Rep: So that would be the equivalent of getting three years worth of updates for the price of one.\n",
            "Customer: Oh, OK.\n",
            "Sales Rep: In addition, special offers like the current promotion don't come around too often.\n",
            "Sales Rep: I would definitely recommend taking advantage of the extra $50 off before it expires.\n",
            "Customer: Yeah, that does sound pretty good.\n",
            "Sales Rep: If I set this order up for you now, it'll ship out today and for $50 less.\n",
            "Sales Rep: Do you have your credit card handy?\n",
            "Sales Rep: And I can place this order for you now.\n",
            "Customer: Yeah, let's go ahead and use a visa.\n",
            "Customer: for number eight.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Call Analysis"
      ],
      "metadata": {
        "id": "ryBscaHMNET4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "def analyze_conversation_metrics(audio_file: str, asr_result_with_speakers: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Analyzes a diarized conversation for key metrics:\n",
        "    - Talk-time ratio per speaker\n",
        "    - Number of questions per speaker\n",
        "    - Longest monologue duration and speaker\n",
        "\n",
        "    Args:\n",
        "        audio_file (str): Path to audio file\n",
        "        asr_result_with_speakers (dict): Output from WhisperX with diarization\n",
        "\n",
        "    Returns:\n",
        "        dict: Metrics summary\n",
        "    \"\"\"\n",
        "    # Audio duration\n",
        "    audio_duration_seconds = len(AudioSegment.from_file(audio_file)) / 1000.0\n",
        "\n",
        "    # Initialize data structures\n",
        "    talk_times = {}\n",
        "    speaker_questions = {}\n",
        "    last_speaker = None\n",
        "    monologue_start_time = 0\n",
        "    longest_monologue_duration = 0\n",
        "    longest_monologue_speaker = \"Unknown\"\n",
        "\n",
        "    # Iterate through segments\n",
        "    for segment in asr_result_with_speakers[\"segments\"]:\n",
        "        speaker = segment.get('speaker', 'Unknown')\n",
        "        start_time_sec = segment.get('start', 0)\n",
        "        end_time_sec = segment.get('end', 0)\n",
        "        text = segment.get('text', \"\")\n",
        "\n",
        "        # 1. Talk-time Ratio\n",
        "        duration = end_time_sec - start_time_sec\n",
        "        talk_times[speaker] = talk_times.get(speaker, 0) + duration\n",
        "\n",
        "        # 2. Number of Questions\n",
        "        if '?' in text or text.lower().strip().endswith(('how?', 'what?', 'where?', 'when?', 'why?')):\n",
        "            speaker_questions[speaker] = speaker_questions.get(speaker, 0) + 1\n",
        "\n",
        "        # 3. Longest Monologue\n",
        "        if speaker != last_speaker:\n",
        "            if last_speaker is not None:\n",
        "                monologue_duration = start_time_sec - monologue_start_time\n",
        "                if monologue_duration > longest_monologue_duration:\n",
        "                    longest_monologue_duration = monologue_duration\n",
        "                    longest_monologue_speaker = last_speaker\n",
        "\n",
        "            # Start new monologue\n",
        "            monologue_start_time = start_time_sec\n",
        "            last_speaker = speaker\n",
        "\n",
        "    # Final monologue check\n",
        "    if last_speaker is not None:\n",
        "        monologue_duration = audio_duration_seconds - monologue_start_time\n",
        "        if monologue_duration > longest_monologue_duration:\n",
        "            longest_monologue_duration = monologue_duration\n",
        "            longest_monologue_speaker = last_speaker\n",
        "\n",
        "    # Normalize talk times into percentages\n",
        "    talk_time_ratios = {\n",
        "        speaker: (duration / audio_duration_seconds) * 100\n",
        "        for speaker, duration in talk_times.items()\n",
        "    }\n",
        "\n",
        "    # Return structured metrics\n",
        "    return {\n",
        "        \"audio_duration_seconds\": audio_duration_seconds,\n",
        "        \"talk_time_ratios\": talk_time_ratios,\n",
        "        \"speaker_questions\": speaker_questions,\n",
        "        \"longest_monologue_duration\": longest_monologue_duration,\n",
        "        \"longest_monologue_speaker\": longest_monologue_speaker\n",
        "    }\n"
      ],
      "metadata": {
        "id": "e5APt4U5gXrt"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = analyze_conversation_metrics(audio_file, result)\n",
        "\n",
        "print(\"\\n--- Sales Call Metrics ---\")\n",
        "print(f\"Audio Duration: {metrics['audio_duration_seconds']:.2f} seconds\")\n",
        "print(f\"1. Talk-time Ratios: {metrics['talk_time_ratios']}\")\n",
        "print(f\"2. Number of Questions: {metrics['speaker_questions']}\")\n",
        "print(f\"3. Longest Monologue Duration: {metrics['longest_monologue_duration']:.2f} seconds by {metrics['longest_monologue_speaker']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNQUGvDz-lpd",
        "outputId": "c248a51d-ac92-4c55-9afe-32760dfc779e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sales Call Metrics ---\n",
            "Audio Duration: 111.65 seconds\n",
            "1. Talk-time Ratios: {'Sales Rep': 51.29424620235024, 'Customer': 31.873387790197743}\n",
            "2. Number of Questions: {'Sales Rep': 6, 'Customer': 2}\n",
            "3. Longest Monologue Duration: 15.98 seconds by Sales Rep\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis"
      ],
      "metadata": {
        "id": "QreTbtyCWEc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary pipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the sentiment analysis model\n",
        "# This model classifies text as POSITIVE or NEGATIVE\n",
        "sentiment_model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Join all transcribed text into a single string for overall sentiment\n",
        "full_transcript_text = \" \".join([seg[\"text\"] for seg in asr_result_with_speakers[\"segments\"]])\n",
        "\n",
        "def analyze_sentiment(full_transcript_text,sentiment_model):\n",
        "  # Analyze the sentiment of the entire transcript\n",
        "  sentiment_result = sentiment_model(full_transcript_text)\n",
        "  sentiment_label = sentiment_result[0][\"label\"]\n",
        "  sentiment_score = sentiment_result[0][\"score\"]\n",
        "\n",
        "  # Condition to set the label to neutral if the score is between 0.4 and 0.6\n",
        "  if 0.4 <= sentiment_score <= 0.6:\n",
        "      sentiment_label = \"NEUTRAL\"\n",
        "\n",
        "  return sentiment_label, sentiment_score\n",
        "\n",
        "sentiment_label,sentiment_score = analyze_sentiment(full_transcript_text,sentiment_model)\n",
        "\n",
        "\n",
        "print(f\"Overall Call Sentiment: {sentiment_label} (Score: {sentiment_score:.2f})\")\n",
        "print(\"Sentiment analysis complete.\")"
      ],
      "metadata": {
        "id": "7H4HjvLTmi1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834fd149-6a44-4532-81de-c5c2eef9783d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Call Sentiment: POSITIVE (Score: 0.70)\n",
            "Sentiment analysis complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actionable Insights"
      ],
      "metadata": {
        "id": "LdwGE-k_ZlUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "def generate_actionable_insights(conversation_string, metrics, sentiment_label, sentiment_score):\n",
        "    try:\n",
        "        # Get the API key from Colab Secrets\n",
        "        GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "        genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while accessing the API key: {e}\\nPlease ensure you have stored your API key in Colab Secrets with the name 'GOOGLE_API_KEY'.\"\n",
        "\n",
        "    # Initialize the Gemini model\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "    # Define the system prompt and the user prompt with the conversation data\n",
        "    prompt = f\"\"\"\n",
        "    You are an AI call analytics assistant.\n",
        "\n",
        "    Here is the call transcript and key metrics:\n",
        "\n",
        "    --- Call Transcript ---\n",
        "    {conversation_string}\n",
        "\n",
        "    --- Call Metrics ---\n",
        "    - Total Duration: {metrics['audio_duration_seconds']:.2f} seconds\n",
        "    - Talk-time Ratios: {metrics['talk_time_ratios']}\n",
        "    - Number of Questions: {metrics['speaker_questions']}\n",
        "    - Overall Sentiment: {sentiment_label} (score: {sentiment_score:.2f})\n",
        "\n",
        "    From this information, provide **exactly ONE actionable insight** that would help improve future calls.\n",
        "    Make it clear, concise, and practical.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the content from the model\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during content generation: {e}\\nPlease check your API key's validity and network connection.\"\n"
      ],
      "metadata": {
        "id": "y2Q4OPV9aMOZ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insights = generate_actionable_insights(conversation_string, metrics, sentiment_label, sentiment_score)\n",
        "print(insights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "-suDGvwk38d_",
        "outputId": "507d6633-3896-420a-ce97-4b75c30bca83"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train sales representatives to address customer price concerns earlier in the call, perhaps by proactively mentioning pricing options or payment plans before delving into the product details.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Timing the complete pipeline"
      ],
      "metadata": {
        "id": "rOObfAam4vo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "pipeline_start = time.time()  # Start timing\n",
        "\n",
        "raw_audio_file = \"/content/downloads/4ostqJD3Psc.wav\"\n",
        "\n",
        "start = time.time()\n",
        "preprocessed_audio = preprocess_audio_librosa(raw_audio_file)\n",
        "print(f\"[✓] Preprocessing complete in {time.time() - start:.2f}s\")\n",
        "\n",
        "start = time.time()\n",
        "asr_result_with_speakers = transcribe_and_diarize(preprocessed_audio, whisper_model, diarize_model)\n",
        "print(f\"[✓] Transcription & diarization complete in {time.time() - start:.2f}s\")\n",
        "\n",
        "start = time.time()\n",
        "conversation_string = format_conversation(asr_result_with_speakers)\n",
        "print(f\"[✓] Formatting complete in {time.time() - start:.2f}s\")\n",
        "\n",
        "start = time.time()\n",
        "metrics = analyze_conversation_metrics(preprocessed_audio, asr_result_with_speakers)\n",
        "print(f\"[✓] Metrics analysis complete in {time.time() - start:.2f}s\")\n",
        "\n",
        "full_transcript_text = \" \".join([seg[\"text\"] for seg in asr_result_with_speakers[\"segments\"]])\n",
        "\n",
        "start = time.time()\n",
        "sentiment_label, sentiment_score = analyze_sentiment(full_transcript_text, sentiment_model)\n",
        "print(f\"[✓] Sentiment analysis complete in {time.time() - start:.2f}s\")\n",
        "\n",
        "start = time.time()\n",
        "insights = generate_actionable_insights(conversation_string, metrics, sentiment_label, sentiment_score)\n",
        "print(f\"[✓] Insight generation complete in {time.time() - start:.2f}s\")\n",
        "\n",
        "# ----------------------------\n",
        "# Final Results\n",
        "# ----------------------------\n",
        "print(\"\\n======= FINAL RESULTS =======\")\n",
        "print(\"📋 Formatted Conversation:\\n\")\n",
        "print(conversation_string)\n",
        "\n",
        "print(\"\\n--- Sales Call Metrics ---\")\n",
        "print(f\"Audio Duration: {metrics['audio_duration_seconds']:.2f} seconds\")\n",
        "print(f\"1. Talk-time Ratios: {metrics['talk_time_ratios']}\")\n",
        "print(f\"2. Number of Questions: {metrics['speaker_questions']}\")\n",
        "print(f\"3. Longest Monologue Duration: {metrics['longest_monologue_duration']:.2f} seconds by {metrics['longest_monologue_speaker']}\")\n",
        "\n",
        "print(\"\\n--- Sentiment ---\")\n",
        "print(f\"Overall Call Sentiment: {sentiment_label} (Score: {sentiment_score:.2f})\")\n",
        "\n",
        "print(\"\\n--- Actionable Insight ---\")\n",
        "print(insights)\n",
        "\n",
        "total_time = time.time() - pipeline_start\n",
        "print(f\"\\n Total pipeline runtime: {total_time:.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fX3ylnl74IWG",
        "outputId": "f3342b8d-fe85-4de9-edbc-f2bd5100d91f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[✓] Preprocessing complete in 0.23s\n",
            "Detected language: en (0.99) in first 30s of audio...\n",
            "[✓] Transcription & diarization complete in 11.07s\n",
            "[✓] Formatting complete in 0.00s\n",
            "[✓] Metrics analysis complete in 0.00s\n",
            "[✓] Sentiment analysis complete in 0.01s\n",
            "[✓] Insight generation complete in 1.86s\n",
            "\n",
            "======= FINAL RESULTS =======\n",
            "📋 Formatted Conversation:\n",
            "\n",
            "Sales Rep: Thank you for calling Nissan.\n",
            "Sales Rep: My name is Lauren.\n",
            "Sales Rep: Can I have your name?\n",
            "Customer: Yeah, my name is John Smith.\n",
            "Sales Rep: Thank you, John.\n",
            "Sales Rep: How can I help you?\n",
            "Customer: I was just calling about to see how much it would cost to update the map in my car.\n",
            "Sales Rep: I'd be happy to help you with that today.\n",
            "Sales Rep: Did you receive a mailer from us?\n",
            "Customer: I did.\n",
            "Customer: Do you need the customer number?\n",
            "Sales Rep: Yes, please.\n",
            "Customer: OK.\n",
            "Customer: It's 15243.\n",
            "Customer: Thank you.\n",
            "Sales Rep: And the year-making model of your vehicle?\n",
            "Customer: Yeah, I have a 2009 Nissan Altima.\n",
            "Sales Rep: Oh, nice car.\n",
            "Customer: Yeah, thank you.\n",
            "Customer: We really enjoy it.\n",
            "Sales Rep: Okay, I think I found your profile here.\n",
            "Sales Rep: Can I have you verify your address and phone number, please?\n",
            "Customer: Yes.\n",
            "Customer: It's 1255 North Research Way.\n",
            "Customer: That's in Orham, Utah, 84097.\n",
            "Customer: And my phone number is 8014311000.\n",
            "Customer: Thanks, John.\n",
            "Sales Rep: I located your information.\n",
            "Sales Rep: The newest version we have available for your vehicle is version 7.7, which was released in March of 2012.\n",
            "Sales Rep: The price of the new map is $99 plus shipping and tax.\n",
            "Sales Rep: Let me go ahead and set up this order for you.\n",
            "Customer: Well, can we wait just a second?\n",
            "Customer: I'm not really sure if I can afford it right now.\n",
            "Sales Rep: All right.\n",
            "Sales Rep: Well, here are a few reasons to consider purchasing today.\n",
            "Sales Rep: It looks as though you haven't updated your vehicle for three years.\n",
            "Sales Rep: So that would be the equivalent of getting three years worth of updates for the price of one.\n",
            "Customer: Oh, OK.\n",
            "Sales Rep: In addition, special offers like the current promotion don't come around too often.\n",
            "Sales Rep: I would definitely recommend taking advantage of the extra $50 off before it expires.\n",
            "Customer: Yeah, that does sound pretty good.\n",
            "Sales Rep: If I set this order up for you now, it'll ship out today and for $50 less.\n",
            "Sales Rep: Do you have your credit card handy?\n",
            "Sales Rep: And I can place this order for you now.\n",
            "Customer: Yeah, let's go ahead and use a visa.\n",
            "Customer: for number eight.\n",
            "\n",
            "--- Sales Call Metrics ---\n",
            "Audio Duration: 111.65 seconds\n",
            "1. Talk-time Ratios: {'Sales Rep': 51.29424620235024, 'Customer': 31.873387790197743}\n",
            "2. Number of Questions: {'Sales Rep': 6, 'Customer': 2}\n",
            "3. Longest Monologue Duration: 15.98 seconds by Sales Rep\n",
            "\n",
            "--- Sentiment ---\n",
            "Overall Call Sentiment: POSITIVE (Score: 0.70)\n",
            "\n",
            "--- Actionable Insight ---\n",
            "Train sales reps to address customer affordability concerns earlier in the call, potentially by qualifying the customer's budget before presenting pricing and promotions.\n",
            "\n",
            "\n",
            " Total pipeline runtime: 13.18s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6dZag__h_s2G"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JuGJkuu_zvw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}